"""
GGUF æ ¼å¼å¯¼å‡ºå·¥å…·ï¼ˆå®˜æ–¹åº“ç‰ˆæœ¬ï¼‰
============================

åŠŸèƒ½è¯´æ˜ï¼š
---------
ä½¿ç”¨ HuggingFace çš„å®˜æ–¹ gguf åº“å°†æ··åˆç²¾åº¦é‡åŒ–é…ç½®å¯¼å‡ºä¸º GGUF æ ¼å¼ï¼Œ
ç¡®ä¿ä¸ llama.cpp å®Œå…¨å…¼å®¹ã€‚

===============================================================
æ··åˆç²¾åº¦é‡åŒ–ç­–ç•¥: W2/W4/W8 + A8 (æƒé‡å¯å˜ä½å®½ + å›ºå®š8ä½æ¿€æ´»)
===============================================================

æœ¬é¡¹ç›®é‡‡ç”¨ W2/W4/W8 + A8 çš„æ··åˆç²¾åº¦é‡åŒ–ç­–ç•¥ï¼š
- æƒé‡ (Weight): æ ¹æ®æ•æ„Ÿåº¦é€‰æ‹© W2, W4, W8
- æ¿€æ´» (Activation): å›ºå®šä½¿ç”¨ A8 (8-bit)

å¯¼å‡ºæ—¶æ¿€æ´»é‡åŒ–ç”±æ¨ç†å¼•æ“ (llama.cpp) åœ¨è¿è¡Œæ—¶å¤„ç†ã€‚

å·¥ä½œæµç¨‹ï¼š
    1. åŠ è½½æ··åˆç²¾åº¦é‡åŒ–é…ç½® (mixed_precision_config.pt)
    2. åŠ è½½åŸå§‹ HuggingFace æ¨¡å‹
    3. æ ¹æ®é…ç½®å¯¹æ¯å±‚è¿›è¡Œ Q4_0/Q8_0 é‡åŒ–
    4. ç”ŸæˆåŒ…å« tokenizer å’Œæ¨¡å‹å…ƒæ•°æ®çš„ GGUF æ–‡ä»¶

é‡åŒ–ç±»å‹æ˜ å°„ï¼š
    - W2 (2-bit) â†’ Q4_0 (ç®€åŒ–å¤„ç†ï¼Œå›  Q2_K å¾ˆå¤æ‚)
    - W4 (4-bit) â†’ Q4_0
    - W8 (8-bit) â†’ Q8_0
    - å…¶ä»–å±‚    â†’ F32 (ä¿æŒç²¾åº¦)

ä½¿ç”¨æ–¹æ³•ï¼š
---------
    # åŸºç¡€ç”¨æ³•
    >>> python export_gguf_official.py
    
    # è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„
    >>> python export_gguf_official.py --output models/my-model.gguf
    
    # æŒ‡å®šé…ç½®æ–‡ä»¶
    >>> python export_gguf_official.py --config my_config.pt --output models/custom.gguf

ä¾èµ–ï¼š
----
    pip install gguf transformers torch huggingface_hub

ä½œè€…ï¼šJiangsheng Yu
"""

import torch
import numpy as np
import os
import argparse
import gc
from pathlib import Path
from typing import Dict, Any
from tqdm import tqdm

from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer

# ä½¿ç”¨å®˜æ–¹ gguf åº“
import gguf


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def get_device() -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¡ç®—è®¾å¤‡
    
    æ³¨æ„ï¼šå¯¼å‡ºè¿‡ç¨‹ä¸»è¦åœ¨ CPU ä¸Šè¿›è¡Œï¼Œæ­¤å‡½æ•°ç”¨äºåŠ è½½æ¨¡å‹
    """
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


# ä½¿ç”¨ gguf åº“è‡ªå¸¦çš„é‡åŒ–å‡½æ•°
from gguf import quants as gguf_quants


def quantize_tensor(weight: np.ndarray, qtype: gguf.GGMLQuantizationType) -> np.ndarray:
    """
    ä½¿ç”¨ gguf åº“çš„é‡åŒ–å‡½æ•°å¯¹å¼ é‡è¿›è¡Œé‡åŒ–
    
    å‚æ•°:
        weight: åŸå§‹æƒé‡å¼ é‡ (np.ndarray)
        qtype: GGML é‡åŒ–ç±»å‹ (Q4_0, Q8_0 ç­‰)
    
    è¿”å›:
        np.ndarray: é‡åŒ–åçš„æ•°æ® (dtype=uint8)
    
    é‡åŒ–æ ¼å¼è¯´æ˜:
        - Q4_0: æ¯32ä¸ªå…ƒç´ ä¸€ä¸ª blockï¼ŒåŒ…å« 1ä¸ª FP16 scale + 16 bytes (32ä¸ª4-bitå€¼)
        - Q8_0: æ¯32ä¸ªå…ƒç´ ä¸€ä¸ª blockï¼ŒåŒ…å« 1ä¸ª FP16 scale + 32 bytes (32ä¸ª8-bitå€¼)
    """
    return gguf_quants.quantize(weight.astype(np.float32), qtype)


def convert_name_hf_to_gguf(name: str) -> str:
    """
    å°† HuggingFace æ¨¡å‹çš„æƒé‡åç§°è½¬æ¢ä¸º GGUF æ ¼å¼
    
    å‚æ•°:
        name: HuggingFace æ ¼å¼çš„æƒé‡åç§°
              ä¾‹: "model.layers.0.self_attn.q_proj.weight"
    
    è¿”å›:
        str: GGUF æ ¼å¼çš„æƒé‡åç§°
             ä¾‹: "blk.0.attn_q.weight"
    
    åç§°æ˜ å°„è§„åˆ™:
        HuggingFace               â†’  GGUF
        ----------------------------------------
        model.                    â†’  (ç§»é™¤)
        layers.N                  â†’  blk.N
        embed_tokens              â†’  token_embd
        input_layernorm           â†’  attn_norm
        post_attention_layernorm  â†’  ffn_norm
        self_attn.q_proj          â†’  attn_q
        self_attn.k_proj          â†’  attn_k
        self_attn.v_proj          â†’  attn_v
        self_attn.o_proj          â†’  attn_output
        mlp.gate_proj             â†’  ffn_gate
        mlp.up_proj               â†’  ffn_up
        mlp.down_proj             â†’  ffn_down
        norm.weight (é¡¶å±‚)        â†’  output_norm.weight
        lm_head.weight            â†’  output.weight
    """
    # å…ˆå¤„ç† model. å‰ç¼€
    name = name.replace("model.", "")
    
    # å¤„ç†å±‚ç¼–å·
    name = name.replace("layers.", "blk.")
    
    # å¤„ç† embed_tokens
    name = name.replace("embed_tokens.weight", "token_embd.weight")
    
    # å¤„ç† layernormï¼ˆæ³¨æ„ï¼šä½¿ç”¨å®Œæ•´æ¨¡å¼é¿å…è¯¯æ›¿æ¢ï¼‰
    name = name.replace(".input_layernorm.weight", ".attn_norm.weight")
    name = name.replace(".post_attention_layernorm.weight", ".ffn_norm.weight")
    
    # å¤„ç† attention ç›¸å…³
    name = name.replace(".self_attn.q_proj.weight", ".attn_q.weight")
    name = name.replace(".self_attn.k_proj.weight", ".attn_k.weight")
    name = name.replace(".self_attn.v_proj.weight", ".attn_v.weight")
    name = name.replace(".self_attn.o_proj.weight", ".attn_output.weight")
    name = name.replace(".self_attn.q_proj.bias", ".attn_q.bias")
    name = name.replace(".self_attn.k_proj.bias", ".attn_k.bias")
    name = name.replace(".self_attn.v_proj.bias", ".attn_v.bias")
    
    # å¤„ç† MLP
    name = name.replace(".mlp.gate_proj.weight", ".ffn_gate.weight")
    name = name.replace(".mlp.up_proj.weight", ".ffn_up.weight")
    name = name.replace(".mlp.down_proj.weight", ".ffn_down.weight")
    
    # å¤„ç†æœ€åçš„ normï¼ˆåªå¤„ç† "norm.weight" å¼€å¤´çš„æƒ…å†µï¼Œä¸æ˜¯ ".attn_norm.weight"ï¼‰
    if name == "norm.weight":
        name = "output_norm.weight"
    
    # lm_head ç‰¹æ®Šå¤„ç†
    if name == "lm_head.weight":
        name = "output.weight"
    
    return name


def export_mixed_precision_gguf_official(
    model_id: str,
    config_path: str,
    output_path: str
):
    """
    ä½¿ç”¨å®˜æ–¹ gguf åº“å¯¼å‡ºæ··åˆç²¾åº¦æ¨¡å‹
    """
    print("\n" + "="*80)
    print("ğŸ”§ æ··åˆç²¾åº¦ GGUF å¯¼å‡º (å®˜æ–¹åº“)")
    print("="*80)
    
    # åŠ è½½é‡åŒ–é…ç½®
    print(f"\nğŸ“„ åŠ è½½é‡åŒ–é…ç½®: {config_path}")
    quant_config = torch.load(config_path, weights_only=False)
    
    # ç»Ÿè®¡
    w2_count = sum(1 for v in quant_config.values() if v['w_bits'] == 2)
    w4_count = sum(1 for v in quant_config.values() if v['w_bits'] == 4)
    w8_count = sum(1 for v in quant_config.values() if v['w_bits'] == 8)
    
    print(f"\nğŸ“Š é‡åŒ–é…ç½®:")
    print(f"   W2å±‚: {w2_count}")
    print(f"   W4å±‚: {w4_count}")
    print(f"   W8å±‚: {w8_count}")
    
    # åŠ è½½æ¨¡å‹é…ç½®
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_id}")
    hf_config = AutoConfig.from_pretrained(model_id)
    
    # åŠ è½½æ¨¡å‹
    print("â³ åŠ è½½æ¨¡å‹æƒé‡...")
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="cpu"
    )
    
    # åŠ è½½ tokenizer è·å–è¯è¡¨ä¿¡æ¯
    print("â³ åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # åˆ›å»º GGUF writer
    print(f"\nğŸ“ åˆ›å»º GGUF æ–‡ä»¶: {output_path}")
    writer = gguf.GGUFWriter(output_path, "qwen2")
    
    # æ·»åŠ æ¨¡å‹å…ƒæ•°æ®
    writer.add_architecture()
    writer.add_name(model_id.split("/")[-1] + "-mixed")
    writer.add_context_length(hf_config.max_position_embeddings)
    writer.add_embedding_length(hf_config.hidden_size)
    writer.add_block_count(hf_config.num_hidden_layers)
    writer.add_feed_forward_length(hf_config.intermediate_size)
    writer.add_head_count(hf_config.num_attention_heads)
    writer.add_head_count_kv(hf_config.num_key_value_heads)
    writer.add_rope_freq_base(hf_config.rope_theta)
    writer.add_layer_norm_rms_eps(hf_config.rms_norm_eps)
    
    # æ·»åŠ å®Œæ•´çš„ tokenizer ä¿¡æ¯
    print("ğŸ“ æ·»åŠ  tokenizer ä¿¡æ¯...")
    
    # ä½¿ç”¨æ¨¡å‹çš„ vocab_sizeï¼ˆæ¯” tokenizer å¤§ï¼Œæœ‰ paddingï¼‰
    model_vocab_size = hf_config.vocab_size  # 152064
    tokenizer_vocab = tokenizer.get_vocab()
    tokenizer_vocab_size = len(tokenizer_vocab)  # 151665
    
    print(f"   æ¨¡å‹ vocab_size: {model_vocab_size}")
    print(f"   Tokenizer vocab_size: {tokenizer_vocab_size}")
    
    # åˆ›å»ºå®Œæ•´çš„è¯è¡¨ï¼ˆå¡«å……åˆ°æ¨¡å‹å¤§å°ï¼‰
    tokens = [""] * model_vocab_size
    scores = [0.0] * model_vocab_size
    token_types = [gguf.TokenType.NORMAL] * model_vocab_size
    
    for token, idx in tokenizer_vocab.items():
        if idx < model_vocab_size:
            tokens[idx] = token
            scores[idx] = -float(idx)
    
    # å¡«å……æœªä½¿ç”¨çš„ token ä½ç½®
    for i in range(tokenizer_vocab_size, model_vocab_size):
        tokens[i] = f"[PAD_{i}]"
        token_types[i] = gguf.TokenType.UNUSED
    
    # è®¾ç½®ç‰¹æ®Š token
    if tokenizer.bos_token_id is not None and tokenizer.bos_token_id < len(token_types):
        token_types[tokenizer.bos_token_id] = gguf.TokenType.CONTROL
    if tokenizer.eos_token_id is not None and tokenizer.eos_token_id < len(token_types):
        token_types[tokenizer.eos_token_id] = gguf.TokenType.CONTROL
    if tokenizer.pad_token_id is not None and tokenizer.pad_token_id < len(token_types):
        token_types[tokenizer.pad_token_id] = gguf.TokenType.CONTROL
    if tokenizer.unk_token_id is not None and tokenizer.unk_token_id < len(token_types):
        token_types[tokenizer.unk_token_id] = gguf.TokenType.UNKNOWN
    
    # æ·»åŠ è¯è¡¨
    writer.add_tokenizer_model("gpt2")
    writer.add_add_bos_token(False)
    writer.add_add_eos_token(False)
    
    # æ·»åŠ  pre-tokenizer ç±»å‹ï¼ˆQwen2 éœ€è¦ï¼‰
    try:
        writer.add_tokenizer_pre("qwen2")
    except:
        # æ—§ç‰ˆæœ¬ gguf å¯èƒ½æ²¡æœ‰è¿™ä¸ªæ–¹æ³•
        pass
    
    writer.add_token_list(tokens)
    writer.add_token_scores(scores)
    writer.add_token_types(token_types)
    
    # æ·»åŠ ç‰¹æ®Š token ID
    if tokenizer.bos_token_id is not None:
        writer.add_bos_token_id(tokenizer.bos_token_id)
    if tokenizer.eos_token_id is not None:
        writer.add_eos_token_id(tokenizer.eos_token_id)
    if tokenizer.pad_token_id is not None:
        writer.add_pad_token_id(tokenizer.pad_token_id)
    
    # æ·»åŠ  chat template
    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
        writer.add_chat_template(tokenizer.chat_template)
    
    # æ·»åŠ  mergesï¼ˆBPE tokenizer éœ€è¦ï¼‰
    try:
        from huggingface_hub import hf_hub_download
        import json
        
        # ä» HuggingFace ä¸‹è½½ tokenizer.json
        tokenizer_json_path = hf_hub_download(model_id, 'tokenizer.json')
        print(f"   Tokenizer JSON: {tokenizer_json_path}")
        
        with open(tokenizer_json_path, 'r') as f:
            tokenizer_json = json.load(f)
        
        if 'model' in tokenizer_json and 'merges' in tokenizer_json['model']:
            merges = tokenizer_json['model']['merges']
            writer.add_token_merges(merges)
            print(f"   âœ… æ·»åŠ äº† {len(merges)} ä¸ª BPE merges")
        else:
            print("   âš ï¸  tokenizer.json ä¸­æ²¡æœ‰ merges")
    except Exception as e:
        print(f"   âš ï¸  æ— æ³•æ·»åŠ  merges: {e}")
    
    # å¤„ç†æƒé‡
    print("\nğŸ”„ é‡åŒ–å¹¶æ·»åŠ æƒé‡...")
    
    total_original_size = 0
    total_quantized_size = 0
    
    for name, param in tqdm(model.named_parameters(), desc="å¤„ç†æƒé‡"):
        weight = param.data.cpu().numpy()
        original_size = weight.nbytes
        total_original_size += original_size
        
        # è½¬æ¢åç§°
        gguf_name = convert_name_hf_to_gguf(name)
        original_shape = weight.shape
        
        # æŸ¥æ‰¾é‡åŒ–é…ç½®
        layer_name = name.replace(".weight", "").replace(".bias", "")
        
        # å†³å®šé‡åŒ–ç±»å‹
        if layer_name in quant_config and ".weight" in name:
            w_bits = quant_config[layer_name]['w_bits']
            
            if w_bits == 2:
                # Q2_K å¤ªå¤æ‚ï¼Œç”¨ Q4_0 æ›¿ä»£
                qtype = gguf.GGMLQuantizationType.Q4_0
                quantized = quantize_tensor(weight, qtype)
                q_str = "Q4_0(W2)"
            elif w_bits == 4:
                qtype = gguf.GGMLQuantizationType.Q4_0
                quantized = quantize_tensor(weight, qtype)
                q_str = "Q4_0"
            else:  # w_bits == 8
                qtype = gguf.GGMLQuantizationType.Q8_0
                quantized = quantize_tensor(weight, qtype)
                q_str = "Q8_0"
            
            total_quantized_size += quantized.nbytes
            
            # æ·»åŠ é‡åŒ–å¼ é‡ - ä½¿ç”¨ gguf åº“çš„é‡åŒ–æ•°æ®ï¼Œä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®š raw_shape
            writer.add_tensor(gguf_name, quantized, raw_dtype=qtype)
        else:
            # éé‡åŒ–å±‚ï¼Œä½¿ç”¨ F32 ä»¥ç¡®ä¿å…¼å®¹æ€§
            weight_f32 = weight.astype(np.float32)
            total_quantized_size += weight_f32.nbytes
            writer.add_tensor(gguf_name, weight_f32, raw_dtype=gguf.GGMLQuantizationType.F32)
        
        # é‡Šæ”¾å†…å­˜
        del weight
    
    # æ¸…ç†æ¨¡å‹
    del model
    gc.collect()
    
    # å†™å…¥æ–‡ä»¶
    print("\nğŸ’¾ å†™å…¥æ–‡ä»¶...")
    writer.write_header_to_file()
    writer.write_kv_data_to_file()
    writer.write_tensors_to_file()
    writer.close()
    
    # è®¡ç®—ç»Ÿè®¡
    file_size = os.path.getsize(output_path)
    compression = total_original_size / file_size if file_size > 0 else 1
    
    print(f"\n{'='*80}")
    print(f"âœ… å¯¼å‡ºå®Œæˆ!")
    print(f"{'='*80}")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"ğŸ“Š åŸå§‹å¤§å°: {total_original_size/1024/1024/1024:.2f} GB")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size/1024/1024/1024:.2f} GB")
    print(f"ğŸ“Š å‹ç¼©æ¯”: {compression:.2f}x")


def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‚æ•°å¹¶æ‰§è¡Œ GGUF å¯¼å‡º
    """
    parser = argparse.ArgumentParser(
        description="æ··åˆç²¾åº¦ GGUF å¯¼å‡ºå·¥å…·ï¼ˆä½¿ç”¨å®˜æ–¹ gguf åº“ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python export_gguf_official.py
  python export_gguf_official.py --output models/custom.gguf
  python export_gguf_official.py --config my_config.pt --output models/my_model.gguf
        """
    )
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="HuggingFace æ¨¡å‹ ID")
    parser.add_argument('--config', type=str, default="mixed_precision_config.pt",
                        help="æ··åˆç²¾åº¦é‡åŒ–é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--output', type=str, default="models/qwen2.5-7b-mixed.gguf",
                        help="è¾“å‡º GGUF æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    os.makedirs(os.path.dirname(args.output) or ".", exist_ok=True)
    
    export_mixed_precision_gguf_official(
        args.model_id,
        args.config,
        args.output
    )


if __name__ == "__main__":
    main()

"""
çœŸå®é‡åŒ–æ¨ç†æµ‹è¯•è„šæœ¬ (Real Quantization Inference Test)
======================================================

ä½¿ç”¨çœŸæ­£çš„ä½ç²¾åº¦è®¡ç®—è¿›è¡Œæ¨ç†ï¼Œè€Œéæ¨¡æ‹Ÿé‡åŒ–ã€‚

æ”¯æŒçš„åç«¯ï¼š
-----------
1. bitsandbytes (CUDA) - 4-bit/8-bit NF4é‡åŒ–
2. PyTorchåŠ¨æ€é‡åŒ– (CPU) - INT8é‡åŒ–
3. AutoGPTQ (CUDA) - GPTQ 2/4-bité‡åŒ–

ä½¿ç”¨æ–¹æ³•ï¼š
---------
>>> pip install bitsandbytes auto-gptq optimum
>>> python real_quant_inference.py --backend bnb4   # 4-bité‡åŒ–
>>> python real_quant_inference.py --backend bnb8   # 8-bité‡åŒ–
>>> python real_quant_inference.py --backend gptq   # GPTQé‡åŒ–
"""

import torch
import time
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig


def get_device() -> str:
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def generate_response(model, tokenizer, prompt: str, device: str, 
                      max_new_tokens: int = 150) -> tuple:
    """ç”Ÿæˆæ¨¡å‹å›å¤å¹¶è¿”å›è€—æ—¶"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer(text, return_tensors="pt")
    if device != "cpu":
        inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # é¢„çƒ­
    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)
    
    # è®¡æ—¶
    if device == "cuda":
        torch.cuda.synchronize()
    
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    if device == "cuda":
        torch.cuda.synchronize()
    
    end_time = time.time()
    elapsed = end_time - start_time
    new_tokens = outputs.shape[1] - inputs['input_ids'].shape[1]
    
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:], 
        skip_special_tokens=True
    )
    
    return response, elapsed, new_tokens


def load_model_fp16(model_id: str, device: str):
    """åŠ è½½FP16åŸå§‹æ¨¡å‹"""
    print("â³ åŠ è½½ FP16 åŸå§‹æ¨¡å‹...")
    
    if device == "cuda":
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            device_map="auto"
        )
    elif device == "mps":
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float32
        ).to("mps")
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float32
        )
    
    return model


def load_model_bnb4(model_id: str):
    """ä½¿ç”¨ bitsandbytes åŠ è½½ 4-bit NF4 é‡åŒ–æ¨¡å‹"""
    print("â³ åŠ è½½ 4-bit NF4 é‡åŒ–æ¨¡å‹ (bitsandbytes)...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,  # åŒé‡é‡åŒ–è¿›ä¸€æ­¥å‹ç¼©
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    return model


def load_model_bnb8(model_id: str):
    """ä½¿ç”¨ bitsandbytes åŠ è½½ 8-bit é‡åŒ–æ¨¡å‹"""
    print("â³ åŠ è½½ 8-bit é‡åŒ–æ¨¡å‹ (bitsandbytes)...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=True,
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    return model


def load_model_gptq(model_id: str):
    """åŠ è½½é¢„é‡åŒ–çš„ GPTQ æ¨¡å‹"""
    print("â³ åŠ è½½ GPTQ é‡åŒ–æ¨¡å‹...")
    
    # å°è¯•åŠ è½½å®˜æ–¹GPTQç‰ˆæœ¬
    gptq_model_id = model_id.replace("Instruct", "Instruct-GPTQ-Int4")
    
    try:
        from auto_gptq import AutoGPTQForCausalLM
        model = AutoGPTQForCausalLM.from_quantized(
            gptq_model_id,
            device_map="auto",
            use_safetensors=True
        )
        return model, gptq_model_id
    except Exception as e:
        print(f"   âš ï¸ AutoGPTQåŠ è½½å¤±è´¥: {e}")
        # å›é€€åˆ°transformersåŠ è½½
        model = AutoModelForCausalLM.from_pretrained(
            gptq_model_id,
            device_map="auto"
        )
        return model, gptq_model_id


def get_model_memory(model) -> float:
    """ä¼°ç®—æ¨¡å‹å†…å­˜å ç”¨ (GB)"""
    total_params = sum(p.numel() for p in model.parameters())
    
    # æ£€æŸ¥é‡åŒ–çŠ¶æ€
    sample_param = next(model.parameters())
    if hasattr(sample_param, 'quant_state'):
        # bitsandbytes é‡åŒ–
        bits = 4 if hasattr(model, 'is_loaded_in_4bit') and model.is_loaded_in_4bit else 8
        memory_gb = total_params * bits / 8 / 1e9
    elif sample_param.dtype == torch.float16:
        memory_gb = total_params * 2 / 1e9
    elif sample_param.dtype == torch.float32:
        memory_gb = total_params * 4 / 1e9
    else:
        memory_gb = total_params * 2 / 1e9
    
    return memory_gb


def run_benchmark(model, tokenizer, model_name: str, device: str, max_tokens: int = 100):
    """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    prompts = [
        "1+1ç­‰äºå¤šå°‘ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿç”¨ä¸€å¥è¯å›ç­”ã€‚",
        "ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°ã€‚",
    ]
    
    print(f"\n{'â”€'*70}")
    print(f"ğŸ“Š {model_name} åŸºå‡†æµ‹è¯•")
    print(f"{'â”€'*70}")
    
    total_time = 0
    total_tokens = 0
    
    for i, prompt in enumerate(prompts, 1):
        response, elapsed, new_tokens = generate_response(
            model, tokenizer, prompt, device, max_new_tokens=max_tokens
        )
        
        total_time += elapsed
        total_tokens += new_tokens
        
        print(f"\n[æµ‹è¯• {i}] {prompt}")
        print(f"å›ç­”: {response[:100]}..." if len(response) > 100 else f"å›ç­”: {response}")
        print(f"â±ï¸  è€—æ—¶: {elapsed:.2f}s | Tokens: {new_tokens} | é€Ÿåº¦: {new_tokens/elapsed:.1f} tok/s")
    
    avg_speed = total_tokens / total_time if total_time > 0 else 0
    
    return {
        "name": model_name,
        "total_time": total_time,
        "total_tokens": total_tokens,
        "avg_speed": avg_speed
    }


def main():
    parser = argparse.ArgumentParser(description="çœŸå®é‡åŒ–æ¨ç†æµ‹è¯•")
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="æ¨¡å‹ID")
    parser.add_argument('--backend', type=str, default="bnb4",
                        choices=["fp16", "bnb4", "bnb8", "gptq", "all"],
                        help="é‡åŒ–åç«¯: fp16(åŸå§‹), bnb4(4-bit), bnb8(8-bit), gptq, all(å…¨éƒ¨å¯¹æ¯”)")
    parser.add_argument('--max_tokens', type=int, default=100,
                        help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    
    args = parser.parse_args()
    device = get_device()
    
    print("\n" + "="*70)
    print("ğŸš€ çœŸå®é‡åŒ–æ¨ç†æµ‹è¯•")
    print("="*70)
    print(f"ğŸ“ è®¾å¤‡: {device}")
    print(f"ğŸ“¦ æ¨¡å‹: {args.model_id}")
    print(f"ğŸ”§ åç«¯: {args.backend}")
    
    # æ£€æŸ¥è®¾å¤‡å…¼å®¹æ€§
    if device != "cuda" and args.backend in ["bnb4", "bnb8", "all"]:
        print("\nâš ï¸  è­¦å‘Š: bitsandbytes ä»…æ”¯æŒ CUDA è®¾å¤‡!")
        print("   å½“å‰è®¾å¤‡:", device)
        print("   å»ºè®®é€‰é¡¹:")
        print("   - ä½¿ç”¨ CUDA GPU")
        print("   - ä½¿ç”¨ --backend gptq (å¦‚æœæœ‰é¢„é‡åŒ–æ¨¡å‹)")
        print("   - ä½¿ç”¨ llama.cpp + GGUF æ ¼å¼ (æ¨èç”¨äº MPS)")
        
        if device == "mps":
            print("\nğŸ’¡ MPS è®¾å¤‡æ¨èæ–¹æ¡ˆ:")
            print("   1. å®‰è£… llama-cpp-python: pip install llama-cpp-python")
            print("   2. ä¸‹è½½ GGUF æ¨¡å‹: huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF")
            print("   3. ä½¿ç”¨ llama.cpp æ¨ç†")
            
            # å°è¯•è¿è¡Œ llama.cpp æ¼”ç¤º
            run_llamacpp_demo(args.model_id)
            return
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
    results = []
    
    backends_to_test = ["fp16", "bnb4", "bnb8"] if args.backend == "all" else [args.backend]
    
    for backend in backends_to_test:
        try:
            if backend == "fp16":
                model = load_model_fp16(args.model_id, device)
            elif backend == "bnb4":
                model = load_model_bnb4(args.model_id)
            elif backend == "bnb8":
                model = load_model_bnb8(args.model_id)
            elif backend == "gptq":
                model, _ = load_model_gptq(args.model_id)
            
            model.eval()
            
            # æ˜¾ç¤ºå†…å­˜å ç”¨
            mem = get_model_memory(model)
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | ä¼°ç®—å†…å­˜: {mem:.2f} GB")
            
            # è¿è¡Œæµ‹è¯•
            result = run_benchmark(model, tokenizer, backend.upper(), device, args.max_tokens)
            results.append(result)
            
            # æ¸…ç†å†…å­˜
            del model
            if device == "cuda":
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"\nâŒ {backend} åŠ è½½å¤±è´¥: {e}")
            continue
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    if len(results) > 1:
        print("\n" + "="*70)
        print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print("="*70)
        print(f"{'æ¨¡å‹':<12} {'æ€»è€—æ—¶(s)':<12} {'æ€»Tokens':<12} {'é€Ÿåº¦(tok/s)':<12} {'åŠ é€Ÿæ¯”':<10}")
        print("-"*70)
        
        baseline_speed = results[0]["avg_speed"] if results else 1
        
        for r in results:
            speedup = r["avg_speed"] / baseline_speed if baseline_speed > 0 else 0
            print(f"{r['name']:<12} {r['total_time']:<12.2f} {r['total_tokens']:<12} {r['avg_speed']:<12.1f} {speedup:<10.2f}x")
    
    print("\n" + "="*70)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("="*70)


def run_llamacpp_demo(model_id: str):
    """è¿è¡Œ llama.cpp æ¼”ç¤ºï¼ˆé€‚ç”¨äº MPSï¼‰"""
    print("\n" + "="*70)
    print("ğŸ¦™ llama.cpp æ¨ç†æ¼”ç¤º (MPS åŠ é€Ÿ)")
    print("="*70)
    
    try:
        from llama_cpp import Llama
        
        # å°è¯•æŸ¥æ‰¾æœ¬åœ° GGUF æ–‡ä»¶
        import os
        import glob
        
        # å¸¸è§çš„ GGUF è·¯å¾„
        possible_paths = [
            "*.gguf",
            "models/*.gguf",
            os.path.expanduser("~/.cache/huggingface/hub/**/qwen*7b*.gguf"),
        ]
        
        gguf_file = None
        for pattern in possible_paths:
            matches = glob.glob(pattern, recursive=True)
            if matches:
                gguf_file = matches[0]
                break
        
        if gguf_file:
            print(f"âœ… æ‰¾åˆ° GGUF æ¨¡å‹: {gguf_file}")
            
            llm = Llama(
                model_path=gguf_file,
                n_ctx=2048,
                n_gpu_layers=-1,  # ä½¿ç”¨æ‰€æœ‰ GPU å±‚
                verbose=False
            )
            
            prompt = "1+1ç­‰äºå¤šå°‘ï¼Ÿ"
            print(f"\né—®é¢˜: {prompt}")
            
            start = time.time()
            output = llm(
                f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n",
                max_tokens=100,
                echo=False
            )
            elapsed = time.time() - start
            
            response = output['choices'][0]['text']
            tokens = output['usage']['completion_tokens']
            
            print(f"å›ç­”: {response}")
            print(f"\nâ±ï¸  è€—æ—¶: {elapsed:.2f}s | Tokens: {tokens} | é€Ÿåº¦: {tokens/elapsed:.1f} tok/s")
            
        else:
            print("\nâš ï¸  æœªæ‰¾åˆ° GGUF æ¨¡å‹æ–‡ä»¶")
            print("\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤ä¸‹è½½:")
            print("1. pip install huggingface-hub")
            print("2. huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF \\")
            print("   qwen2.5-7b-instruct-q4_k_m.gguf --local-dir ./models")
            print("3. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
            
    except ImportError:
        print("\nâš ï¸  llama-cpp-python æœªå®‰è£…")
        print("è¯·è¿è¡Œ: pip install llama-cpp-python")
        print("\nå¯¹äº Apple Silicon, ä½¿ç”¨ Metal åŠ é€Ÿ:")
        print("CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python")


if __name__ == "__main__":
    main()

"""
混合精度 PTQ 主程序 (Mixed-Precision Post-Training Quantization)
================================================================

⚠️ 重要说明：
-----------
本程序使用【模拟量化】来搜索最优的逐层位宽配置。
模拟量化不会加速推理，仅用于验证量化精度。
如需真正的加速效果，请使用 compare_real_quant.py 进行真实量化推理。

工作流程：
---------
1. 加载预训练模型和校准数据
2. 敏感度分析：评估每层对 W2/W4/W8 的敏感程度
3. 遗传算法优化：在压缩率和精度之间寻找最优配置
4. 保存混合精度配置到文件

核心技术：
---------
- SmoothQuant: 通过激活值平滑减少量化难度
- 层敏感度分析: 识别可承受激进压缩的层
- 遗传算法: 全局搜索最优位宽配置

===============================================================
混合精度量化策略: W2/W4/W8 + A8 (权重可变位宽 + 固定8位激活)
===============================================================

┌─────────────────────────────────────────────────────────────────────┐
│  权重量化 (Weight Quantization) - 可变位宽                          │
├─────────────────────────────────────────────────────────────────────┤
│  • W2 (2-bit): 低敏感层，最大压缩 (1/8 原始大小)                    │
│  • W4 (4-bit): 中敏感层，平衡压缩 (1/4 原始大小)                    │
│  • W8 (8-bit): 高敏感层，保持精度 (1/2 原始大小)                    │
│  使用对称量化 + 分组量化 (group_size=128)                           │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  激活量化 (Activation Quantization) - 固定 A8                       │
├─────────────────────────────────────────────────────────────────────┤
│  • A8 (8-bit): 所有层统一使用 8-bit 激活量化                        │
│  • 使用非对称量化 (per-tensor)                                      │
│  • 激活值分布通常不对称，非对称量化效果更好                         │
└─────────────────────────────────────────────────────────────────────┘

策略总结：
---------
- 低敏感层 → W2 + A8: 最大压缩
- 中敏感层 → W4 + A8: 平衡压缩
- 高敏感层 → W8 + A8: 保持精度
- 激活值全部使用 A8，简化硬件实现并保证精度

使用方法：
---------
>>> python mixed_precision_ptq.py
>>> python mixed_precision_ptq.py --model_id Qwen/Qwen2.5-7B-Instruct --device mps

输出：
-----
- mixed_precision_config.pt: 包含每层量化配置的字典

下一步：
-------
配置搜索完成后，使用真实量化推理获得加速效果：
>>> python compare_real_quant.py
"""

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import argparse
from tqdm import tqdm
import numpy as np

from data_utils import get_calib_dataset, create_mock_input
from quant_utils import quantize_tensor
from genetic_optim import MixedPrecisionGA, LayerSensitivityAnalyzer


def get_best_device() -> str:
    """
    自动检测最佳可用设备
    
    优先级: CUDA > MPS (Apple Silicon) > CPU
    
    返回：
    ------
    str
        设备名称: 'cuda', 'mps', 或 'cpu'
    """
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def load_model(model_id: str, device: str):
    """
    加载预训练模型
    
    参数：
    -----
    model_id : str
        HuggingFace模型ID或本地路径
    device : str
        目标设备
    
    返回：
    ------
    model : AutoModelForCausalLM
        加载的模型
    tokenizer : AutoTokenizer
        对应的分词器
    """
    print(f"正在加载模型: {model_id}")
    print(f"目标设备: {device}")
    
    if device == "cuda":
        model = AutoModelForCausalLM.from_pretrained(
            model_id, 
            torch_dtype=torch.float16, 
            device_map="auto"
        )
    elif device == "mps":
        # MPS目前对float16支持有限，使用float32
        model = AutoModelForCausalLM.from_pretrained(
            model_id, 
            torch_dtype=torch.float32
        )
        model = model.to("mps")
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_id, 
            torch_dtype=torch.float32
        )
    
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    
    return model, tokenizer


def get_linear_layers(model) -> list:
    """
    获取模型中所有需要量化的线性层
    
    只选择decoder layers中的线性层，跳过embedding和lm_head
    
    参数：
    -----
    model : AutoModelForCausalLM
        目标模型
    
    返回：
    ------
    list
        [(层名称, 层模块), ...] 列表
    """
    layers = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and "layers" in name:
            layers.append((name, module))
    return layers


def evaluate_layer_sensitivity(layer, calib_input, device) -> dict:
    """
    评估单层对不同量化位宽的敏感度
    
    参数：
    -----
    layer : nn.Linear
        待评估的线性层
    calib_input : torch.Tensor
        校准输入，shape: [batch, seq_len, in_features]
    device : str
        计算设备
    
    返回：
    ------
    dict
        位宽到MSE的映射: {2: mse_w2, 4: mse_w4, 8: mse_w8}
    """
    # 获取原始FP输出
    with torch.no_grad():
        original_output = layer(calib_input)
    
    sensitivity = {}
    for n_bits in [2, 4, 8]:
        w = layer.weight
        
        # 裁剪权重（减少outlier影响）
        limit = w.abs().amax() * 0.9
        w_clipped = torch.clamp(w, -limit, limit)
        
        # 量化权重
        w_q = quantize_tensor(w_clipped, n_bits=n_bits, group_size=128, sym=True)
        
        # 量化激活（固定A8）
        x_q = quantize_tensor(calib_input, n_bits=8, group_size=-1, sym=False)
        
        # 计算量化后输出
        with torch.no_grad():
            out_q = torch.nn.functional.linear(x_q, w_q, layer.bias)
        
        # 计算MSE
        mse = torch.mean((out_q - original_output) ** 2).item()
        sensitivity[n_bits] = mse
    
    return sensitivity


def create_fitness_function(layers_to_quantize: list, sensitivities: dict):
    """
    创建遗传算法的适应度函数
    
    适应度 = -总MSE（MSE越小，适应度越高）
    
    参数：
    -----
    layers_to_quantize : list
        待量化层列表
    sensitivities : dict
        预计算的敏感度字典
    
    返回：
    ------
    Callable
        适应度函数
    """
    def fitness_function(bit_config):
        total_mse = 0
        for i, (name, _) in enumerate(layers_to_quantize):
            bits = int(bit_config[i])
            mse = sensitivities[name].get(bits, sensitivities[name][4])
            total_mse += mse
        return -total_mse  # 负MSE作为适应度
    
    return fitness_function


def save_config(layers_to_quantize: list, best_config: np.ndarray, 
                output_path: str) -> dict:
    """
    保存混合精度配置
    
    参数：
    -----
    layers_to_quantize : list
        层列表
    best_config : np.ndarray
        最优位宽配置
    output_path : str
        输出文件路径
    
    返回：
    ------
    dict
        配置字典
    """
    mixed_config = {}
    w2_layers, w4_layers, w8_layers = [], [], []
    
    for i, (name, _) in enumerate(layers_to_quantize):
        bits = int(best_config[i])
        mixed_config[name] = {
            'w_bits': bits,
            'a_bits': 8,  # 固定A8
            'clip_ratio': 0.7 if bits == 2 else 0.9,  # W2使用更激进的裁剪
            'smooth_alpha': 0.5
        }
        
        if bits == 2:
            w2_layers.append(name)
        elif bits == 4:
            w4_layers.append(name)
        else:
            w8_layers.append(name)
    
    # 打印配置摘要
    print(f"\n{'='*60}")
    print("混合精度配置摘要")
    print('='*60)
    
    print(f"\nW2层 ({len(w2_layers)}个):")
    for name in w2_layers[:5]:
        print(f"  - {name}")
    if len(w2_layers) > 5:
        print(f"  ... 及其他 {len(w2_layers) - 5} 层")
    
    print(f"\nW4层 ({len(w4_layers)}个):")
    for name in w4_layers[:5]:
        print(f"  - {name}")
    if len(w4_layers) > 5:
        print(f"  ... 及其他 {len(w4_layers) - 5} 层")
    
    print(f"\nW8层 ({len(w8_layers)}个):")
    for name in w8_layers[:5]:
        print(f"  - {name}")
    if len(w8_layers) > 5:
        print(f"  ... 及其他 {len(w8_layers) - 5} 层")
    
    # 计算压缩统计
    n_layers = len(layers_to_quantize)
    total_bits_orig = n_layers * 16  # 假设原始FP16
    total_bits_quant = sum(best_config)
    compression = total_bits_quant / total_bits_orig
    
    print(f"\n{'='*60}")
    print("压缩统计")
    print('='*60)
    print(f"  总层数: {n_layers}")
    print(f"  W2: {len(w2_layers)} 层")
    print(f"  W4: {len(w4_layers)} 层")
    print(f"  W8: {len(w8_layers)} 层")
    print(f"  压缩比: {compression:.1%} (原始大小的 {compression*100:.1f}%)")
    print(f"  内存节省: {(1-compression)*100:.1f}%")
    print('='*60)
    
    # 保存配置
    torch.save(mixed_config, output_path)
    print(f"\n✓ 配置已保存至: {output_path}")
    
    return mixed_config


def main():
    """主程序入口"""
    parser = argparse.ArgumentParser(
        description="混合精度PTQ量化 - 基于遗传算法优化",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
示例:
  # 基本用法 (自动检测设备)
  python mixed_precision_ptq.py
  
  # 指定模型和设备
  python mixed_precision_ptq.py --model_id Qwen/Qwen2.5-7B-Instruct --device cuda
  
  # 完整参数
  python mixed_precision_ptq.py \\
      --model_id Qwen/Qwen2.5-7B-Instruct \\
      --device mps \\
      --n_layers 196 \\
      --ga_pop 20 \\
      --ga_gen 15 \\
      --target_compression 0.25 \\
      --output my_config.pt
        """
    )
    
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="HuggingFace模型ID或本地路径")
    parser.add_argument('--device', type=str, default=get_best_device(),
                        help="计算设备: cuda, mps, cpu")
    parser.add_argument('--n_samples', type=int, default=64,
                        help="校准样本数量")
    parser.add_argument('--n_layers', type=int, default=196,
                        help="要量化的层数 (Qwen2.5-7B共196层)")
    parser.add_argument('--ga_pop', type=int, default=20,
                        help="遗传算法种群大小")
    parser.add_argument('--ga_gen', type=int, default=12,
                        help="遗传算法迭代代数")
    parser.add_argument('--target_compression', type=float, default=0.25,
                        help="目标压缩比 (0.25表示原大小的25%%)")
    parser.add_argument('--output', type=str, default="mixed_precision_config.pt",
                        help="输出配置文件路径")
    
    args = parser.parse_args()
    
    # 打印配置
    print("="*60)
    print("混合精度PTQ量化")
    print("="*60)
    print(f"模型: {args.model_id}")
    print(f"设备: {args.device}")
    print(f"目标层数: {args.n_layers}")
    print(f"目标压缩比: {args.target_compression:.0%}")
    print("="*60 + "\n")
    
    # Step 1: 加载模型
    model, tokenizer = load_model(args.model_id, args.device)
    
    # 获取所有线性层
    all_layers = get_linear_layers(model)
    layers_to_quantize = all_layers[:args.n_layers]
    n_layers = len(layers_to_quantize)
    print(f"\n将对 {n_layers} 个线性层进行量化分析\n")
    
    # Step 2: 敏感度分析
    print("="*60)
    print("步骤1: 层敏感度分析")
    print("="*60)
    
    sensitivities = {}
    for name, layer in tqdm(layers_to_quantize, desc="分析敏感度"):
        # 创建模拟输入
        mock_input = create_mock_input(
            layer, 
            batch_size=1, 
            seq_len=128,
            device=layer.weight.device,
            dtype=layer.weight.dtype
        )
        
        sens = evaluate_layer_sensitivity(layer, mock_input, args.device)
        sensitivities[name] = sens
        
        # 分类显示
        w2_mse = sens[2]
        if w2_mse < 0.1:
            category = "低敏感度(可用W2)"
        elif w2_mse < 0.5:
            category = "中敏感度(建议W4)"
        else:
            category = "高敏感度(保持W8)"
        
        # 只显示部分层的详细信息
        if len(sensitivities) <= 10 or len(sensitivities) % 20 == 0:
            print(f"  {name}: W2={w2_mse:.4f}, W4={sens[4]:.4f} -> {category}")
    
    # Step 3: 遗传算法优化
    print("\n" + "="*60)
    print("步骤2: 遗传算法优化")
    print("="*60)
    
    fitness_func = create_fitness_function(layers_to_quantize, sensitivities)
    
    ga = MixedPrecisionGA(
        n_layers=n_layers,
        population_size=args.ga_pop,
        n_generations=args.ga_gen,
        mutation_rate=0.15
    )
    
    best_config = ga.optimize(fitness_func, target_compression=args.target_compression)
    
    # Step 4: 保存配置
    print("\n" + "="*60)
    print("步骤3: 生成最终配置")
    print("="*60)
    
    save_config(layers_to_quantize, best_config, args.output)
    
    print("\n✓ 混合精度PTQ完成!")
    print(f"  下一步: 使用 'python test_simulated_quant.py' 测试模拟量化效果")
    print(f"          或使用 'python export_gguf_official.py' 导出GGUF格式")


if __name__ == "__main__":
    main()

"""
æ¨¡æ‹Ÿé‡åŒ–å¯¹æ¯”æµ‹è¯• (Simulated Quantization Test)
==============================================

âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯ã€æ¨¡æ‹Ÿé‡åŒ–ã€‘æµ‹è¯•ï¼Œç”¨äºéªŒè¯é‡åŒ–ç²¾åº¦ï¼Œä¸èƒ½è·å¾—åŠ é€Ÿæ•ˆæœï¼
        å¦‚éœ€çœŸæ­£çš„åŠ é€Ÿï¼Œè¯·ä½¿ç”¨ compare_real_quant.py

===============================================================
æ··åˆç²¾åº¦é‡åŒ–ç­–ç•¥: W2/W4/W8 + A8 (æƒé‡å¯å˜ä½å®½ + å›ºå®š8ä½æ¿€æ´»)
===============================================================

æœ¬è„šæœ¬å¯¹æ¯”ã€åŸå§‹æ¨¡å‹ã€‘ä¸ã€æ¨¡æ‹Ÿé‡åŒ–æ¨¡å‹ã€‘çš„æ¨ç†æ•ˆæœï¼ŒéªŒè¯é‡åŒ–é…ç½®çš„ç²¾åº¦å½±å“ã€‚

æ¨¡æ‹Ÿé‡åŒ– vs çœŸå®é‡åŒ–ï¼š
--------------------
  æ¨¡æ‹Ÿé‡åŒ–: FP32 â†’ é‡åŒ–(round) â†’ åé‡åŒ– â†’ FP32
    â€¢ æ•°æ®ç±»å‹å§‹ç»ˆæ˜¯ FP32ï¼Œåªæ¨¡æ‹Ÿç²¾åº¦æŸå¤±
    â€¢ âŒ ä¸ä¼šåŠ é€Ÿï¼ˆåè€Œæ›´æ…¢ï¼‰
    â€¢ âœ… ç”¨äºè¯„ä¼°é‡åŒ–é…ç½®å¯¹ç²¾åº¦çš„å½±å“

  çœŸå®é‡åŒ–: FP32 â†’ INT4/INT8 â†’ GGUFæ ¼å¼
    â€¢ ä½¿ç”¨ä½ç²¾åº¦æ•´æ•°è¿ç®—
    â€¢ âœ… æ¨ç†åŠ é€Ÿ 5-10x
    â€¢ âœ… ä½¿ç”¨ compare_real_quant.py æµ‹è¯•

ä½¿ç”¨æ–¹æ³•ï¼š
---------
  # åŸºç¡€æµ‹è¯•
  python test_simulated_quant.py
  
  # è‡ªå®šä¹‰æç¤º
  python test_simulated_quant.py --prompt "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
  
  # æŒ‡å®šé…ç½®æ–‡ä»¶
  python test_simulated_quant.py --config my_config.pt --max_tokens 300
"""

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import argparse
import time
import copy
from quant_utils import MixedPrecisionLinear


def get_device() -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡
    
    ä¼˜å…ˆçº§: CUDA > MPS (Apple Silicon) > CPU
    """
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def apply_mixed_precision(model, config: dict) -> tuple:
    """
    å°†æ··åˆç²¾åº¦é‡åŒ–é…ç½®åº”ç”¨åˆ°æ¨¡å‹
    
    éå†é…ç½®ä¸­çš„æ¯ä¸ªå±‚ï¼Œå°†åŸå§‹ nn.Linear æ›¿æ¢ä¸º MixedPrecisionLinear
    
    å‚æ•°:
        model: HuggingFace æ¨¡å‹
        config: é‡åŒ–é…ç½®å­—å…¸
    
    è¿”å›:
        (æ¨¡å‹, ç»Ÿè®¡ä¿¡æ¯å­—å…¸)
    """
    stats = {'W2': 0, 'W4': 0, 'W8': 0}
    
    for name, params in config.items():
        parts = name.split('.')
        parent = model
        
        try:
            for part in parts[:-1]:
                parent = getattr(parent, part)
            layer_name = parts[-1]
            original = getattr(parent, layer_name)
            
            if isinstance(original, nn.Linear):
                new_layer = MixedPrecisionLinear(
                    original,
                    w_bits=params['w_bits'],
                    a_bits=params['a_bits'],
                    clip_ratio=params['clip_ratio'],
                    smooth_alpha=params['smooth_alpha']
                )
                setattr(parent, layer_name, new_layer)
                
                if params['w_bits'] == 2:
                    stats['W2'] += 1
                elif params['w_bits'] == 4:
                    stats['W4'] += 1
                else:
                    stats['W8'] += 1
                    
        except Exception as e:
            pass
    
    return model, stats


def generate_response(model, tokenizer, prompt: str, device: str, 
                      max_new_tokens: int = 150) -> tuple:
    """
    ç”Ÿæˆæ¨¡å‹å›å¤å¹¶è¿”å›è€—æ—¶
    
    è¿”å›ï¼š(å›å¤å†…å®¹, ç”Ÿæˆæ—¶é—´ç§’, ç”Ÿæˆçš„tokenæ•°)
    """
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer(text, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # è®¡æ—¶
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    end_time = time.time()
    elapsed = end_time - start_time
    
    # è®¡ç®—ç”Ÿæˆçš„tokenæ•°
    new_tokens = outputs.shape[1] - inputs['input_ids'].shape[1]
    
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:], 
        skip_special_tokens=True
    )
    
    return response, elapsed, new_tokens


def print_comparison(prompt: str, original_result: tuple, quant_result: tuple, idx: int):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    orig_response, orig_time, orig_tokens = original_result
    quant_response, quant_time, quant_tokens = quant_result
    
    print(f"\n{'='*80}")
    print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹ {idx}")
    print(f"{'='*80}")
    print(f"\nğŸ”¹ é—®é¢˜: {prompt}")
    
    print(f"\n{'â”€'*80}")
    print(f"ğŸ”µ ã€åŸå§‹æ¨¡å‹ã€‘ Qwen2.5-7B-Instruct")
    print(f"{'â”€'*80}")
    print(f"{orig_response}")
    print(f"\n   â±ï¸  è€—æ—¶: {orig_time:.2f}s | Tokens: {orig_tokens} | é€Ÿåº¦: {orig_tokens/orig_time:.1f} tokens/s")
    
    print(f"\n{'â”€'*80}")
    print(f"ğŸŸ¢ ã€é‡åŒ–æ¨¡å‹ã€‘ Mixed-Precision (W2/W4/W8)")
    print(f"{'â”€'*80}")
    print(f"{quant_response}")
    print(f"\n   â±ï¸  è€—æ—¶: {quant_time:.2f}s | Tokens: {quant_tokens} | é€Ÿåº¦: {quant_tokens/quant_time:.1f} tokens/s")
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    speedup = orig_time / quant_time if quant_time > 0 else 0
    print(f"\n   ğŸ“Š åŠ é€Ÿæ¯”: {speedup:.2f}x")


def main():
    """ä¸»ç¨‹åº"""
    parser = argparse.ArgumentParser(
        description="å¯¹æ¯”åŸå§‹æ¨¡å‹ä¸é‡åŒ–æ¨¡å‹çš„æ¨ç†æ•ˆæœ",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="HuggingFaceæ¨¡å‹ID")
    parser.add_argument('--config', type=str, default="mixed_precision_config.pt",
                        help="æ··åˆç²¾åº¦é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--prompt', type=str, default=None,
                        help="è‡ªå®šä¹‰æµ‹è¯•æç¤ºï¼ˆå¯é€‰ï¼‰")
    parser.add_argument('--max_tokens', type=int, default=200,
                        help="æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤ 200ï¼‰")
    
    args = parser.parse_args()
    
    device = get_device()
    
    print("\n" + "="*80)
    print("ğŸ”¬ Qwen2.5-7B æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("   åŸå§‹æ¨¡å‹ vs æ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹")
    print("="*80)
    print(f"\nğŸ“ è®¾å¤‡: {device}")
    print(f"ğŸ“¦ æ¨¡å‹: {args.model_id}")
    print(f"ğŸ“„ é…ç½®: {args.config}")
    
    # ========== åŠ è½½åŸå§‹æ¨¡å‹ ==========
    print("\n" + "â”€"*80)
    print("â³ æ­£åœ¨åŠ è½½åŸå§‹æ¨¡å‹...")
    
    if device == "mps":
        original_model = AutoModelForCausalLM.from_pretrained(
            args.model_id, 
            torch_dtype=torch.float32
        )
        original_model = original_model.to("mps")
    else:
        original_model = AutoModelForCausalLM.from_pretrained(
            args.model_id, 
            torch_dtype=torch.float16, 
            device_map="auto"
        )
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
    original_model.eval()
    print("âœ… åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # ========== åŠ è½½é‡åŒ–æ¨¡å‹ ==========
    print("\nâ³ æ­£åœ¨åŠ è½½é‡åŒ–æ¨¡å‹...")
    
    if device == "mps":
        quant_model = AutoModelForCausalLM.from_pretrained(
            args.model_id, 
            torch_dtype=torch.float32
        )
        quant_model = quant_model.to("mps")
    else:
        quant_model = AutoModelForCausalLM.from_pretrained(
            args.model_id, 
            torch_dtype=torch.float16, 
            device_map="auto"
        )
    
    # åº”ç”¨æ··åˆç²¾åº¦é…ç½®
    try:
        config = torch.load(args.config, map_location='cpu')
        quant_model, stats = apply_mixed_precision(quant_model, config)
        
        print("âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   ğŸ“Š é‡åŒ–å±‚ç»Ÿè®¡: W2={stats['W2']}, W4={stats['W4']}, W8={stats['W8']}")
        
        total = stats['W2'] + stats['W4'] + stats['W8']
        bits_total = stats['W2'] * 2 + stats['W4'] * 4 + stats['W8'] * 8
        bits_orig = total * 16
        compression = bits_total / bits_orig if bits_orig > 0 else 1
        print(f"   ğŸ’¾ å‹ç¼©æ¯”: {compression:.1%} | å†…å­˜èŠ‚çœ: {(1-compression)*100:.1f}%")
        
    except FileNotFoundError:
        print(f"\nâŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {args.config}")
        print("   è¯·å…ˆè¿è¡Œ: python mixed_precision_ptq.py")
        return
    
    quant_model.eval()
    
    # ========== æµ‹è¯•ç”¨ä¾‹ ==========
    if args.prompt:
        prompts = [args.prompt]
    else:
        prompts = [
            # åŸºç¡€æ•°å­¦
            "è®¡ç®— 123 Ã— 456 = ?",
            
            # çŸ¥è¯†é—®ç­”
            "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹å¤ªé˜³ç³»çš„å…«å¤§è¡Œæ˜Ÿã€‚",
            
            # é€»è¾‘æ¨ç†
            "å°æ˜æ¯”å°çº¢å¤§3å²ï¼Œå°çº¢ä»Šå¹´10å²ï¼Œè¯·é—®å°æ˜5å¹´åå¤šå°‘å²ï¼Ÿ",
            
            # ä»£ç ç”Ÿæˆ
            "ç”¨Pythonå®ç°ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾ç®—æ³•ï¼Œè¦æ±‚æœ‰è¯¦ç»†æ³¨é‡Šã€‚",
            
            # åˆ›æ„å†™ä½œ
            "è¯·ç”¨ä¸€å¥è¯æè¿°äººå·¥æ™ºèƒ½çš„æœªæ¥ã€‚",
            
            # è‹±æ–‡ç†è§£
            "Translate the following to Chinese: 'The quick brown fox jumps over the lazy dog.'",
            
            # ä¸“ä¸šçŸ¥è¯†
            "ä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿè¯·ç®€è¦è¯´æ˜å…¶æ ¸å¿ƒæœºåˆ¶ã€‚",
            
            # å¸¸è¯†æ¨ç†
            "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿç”¨ç®€å•çš„è¯­è¨€è§£é‡Šã€‚",
        ]
    
    print("\n" + "="*80)
    print("ğŸš€ å¼€å§‹å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    # æ”¶é›†ç»Ÿè®¡æ•°æ®
    total_orig_time = 0
    total_quant_time = 0
    total_orig_tokens = 0
    total_quant_tokens = 0
    
    for idx, prompt in enumerate(prompts, 1):
        # åŸå§‹æ¨¡å‹æ¨ç†
        orig_result = generate_response(
            original_model, tokenizer, prompt, device, 
            max_new_tokens=args.max_tokens
        )
        
        # é‡åŒ–æ¨¡å‹æ¨ç†
        quant_result = generate_response(
            quant_model, tokenizer, prompt, device, 
            max_new_tokens=args.max_tokens
        )
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        print_comparison(prompt, orig_result, quant_result, idx)
        
        # ç´¯è®¡ç»Ÿè®¡
        total_orig_time += orig_result[1]
        total_quant_time += quant_result[1]
        total_orig_tokens += orig_result[2]
        total_quant_tokens += quant_result[2]
    
    # ========== æ€»ç»“ç»Ÿè®¡ ==========
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    print(f"\nğŸ”µ åŸå§‹æ¨¡å‹:")
    print(f"   æ€»è€—æ—¶: {total_orig_time:.2f}s")
    print(f"   æ€»Tokens: {total_orig_tokens}")
    print(f"   å¹³å‡é€Ÿåº¦: {total_orig_tokens/total_orig_time:.1f} tokens/s")
    
    print(f"\nğŸŸ¢ é‡åŒ–æ¨¡å‹:")
    print(f"   æ€»è€—æ—¶: {total_quant_time:.2f}s")
    print(f"   æ€»Tokens: {total_quant_tokens}")
    print(f"   å¹³å‡é€Ÿåº¦: {total_quant_tokens/total_quant_time:.1f} tokens/s")
    
    avg_speedup = total_orig_time / total_quant_time if total_quant_time > 0 else 0
    print(f"\nğŸ“ˆ å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
    print(f"ğŸ’¾ å†…å­˜èŠ‚çœ: {(1-compression)*100:.1f}%")
    
    print("\n" + "="*80)
    print("âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()

"""
çœŸå®é‡åŒ–å¯¹æ¯”æµ‹è¯• (Real Quantization Comparison)
===============================================

æœ¬è„šæœ¬å¯¹æ¯”ã€åŸå§‹æ¨¡å‹ã€‘ä¸ã€çœŸå®é‡åŒ–æ¨¡å‹ã€‘çš„æ¨ç†æ€§èƒ½ã€‚

âš ï¸ é‡è¦è¯´æ˜ï¼š
-----------
è¿™æ˜¯çœŸå®é‡åŒ–æµ‹è¯•ï¼Œä½¿ç”¨ llama.cpp è¿›è¡ŒçœŸæ­£çš„ä½ç²¾åº¦æ¨ç†ï¼ˆINT4ï¼‰ã€‚
ä¸æ¨¡æ‹Ÿé‡åŒ–ä¸åŒï¼ŒçœŸå®é‡åŒ–å¯ä»¥è·å¾—å®é™…çš„åŠ é€Ÿæ•ˆæœï¼

å…¸å‹ç»“æœï¼š
---------
- æ¨ç†é€Ÿåº¦ï¼šæå‡ 5-10 å€
- å†…å­˜å ç”¨ï¼šå‡å°‘ 70-85%
- å›ç­”è´¨é‡ï¼šæ¥è¿‘åŸå§‹æ¨¡å‹

æ”¯æŒçš„åŠ é€Ÿåç«¯ï¼š
--------------
- macOS: Metal (Apple Silicon GPU)
- Linux/Windows: CUDA (NVIDIA GPU)
- CPU: æ‰€æœ‰å¹³å°

ä½¿ç”¨æ–¹æ³•ï¼š
---------
# é»˜è®¤æµ‹è¯•ï¼ˆéœ€è¦å…ˆä¸‹è½½ GGUF æ¨¡å‹ï¼‰
>>> python compare_real_quant.py

# è‡ªå®šä¹‰æµ‹è¯•
>>> python compare_real_quant.py --max_tokens 200

# ä¸‹è½½ GGUF æ¨¡å‹
>>> huggingface-cli download bartowski/Qwen2.5-7B-Instruct-GGUF \\
...     Qwen2.5-7B-Instruct-Q4_K_M.gguf --local-dir models
"""

import torch
import time
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer


def get_device() -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡
    
    ä¼˜å…ˆçº§: CUDA > MPS (Apple Silicon) > CPU
    """
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def generate_with_transformers(model, tokenizer, prompt: str, device: str, 
                                max_new_tokens: int = 100) -> tuple:
    """
    ä½¿ç”¨ Transformers ç”Ÿæˆå›å¤ï¼ˆåŸå§‹æ¨¡å‹ï¼‰
    
    å‚æ•°:
        model: HuggingFace æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        prompt: ç”¨æˆ·è¾“å…¥
        device: è®¡ç®—è®¾å¤‡
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
    
    è¿”å›:
        (å›å¤å†…å®¹, è€—æ—¶ç§’æ•°, ç”Ÿæˆçš„tokenæ•°)
    """
    # æ„å»ºå¯¹è¯æ ¼å¼
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer(text, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # é¢„çƒ­ï¼ˆè®© GPU è¿›å…¥å·¥ä½œçŠ¶æ€ï¼‰
    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=3, pad_token_id=tokenizer.eos_token_id)
    
    # æ­£å¼æ¨ç†å¹¶è®¡æ—¶
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # è´ªå©ªè§£ç ï¼Œç»“æœå¯å¤ç°
            pad_token_id=tokenizer.eos_token_id
        )
    
    elapsed = time.time() - start_time
    new_tokens = outputs.shape[1] - inputs['input_ids'].shape[1]
    
    # è§£ç è¾“å‡º
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:], 
        skip_special_tokens=True
    )
    
    return response, elapsed, new_tokens


def generate_with_llamacpp(llm, prompt: str, max_new_tokens: int = 100) -> tuple:
    """
    ä½¿ç”¨ llama.cpp ç”Ÿæˆå›å¤ï¼ˆçœŸå®é‡åŒ–æ¨¡å‹ï¼‰
    
    llama.cpp ä½¿ç”¨çœŸæ­£çš„ä½ç²¾åº¦æ•´æ•°è¿ç®—ï¼Œå¯ä»¥è·å¾—å®é™…åŠ é€Ÿã€‚
    
    å‚æ•°:
        llm: llama_cpp.Llama æ¨¡å‹å®ä¾‹
        prompt: ç”¨æˆ·è¾“å…¥
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
    
    è¿”å›:
        (å›å¤å†…å®¹, è€—æ—¶ç§’æ•°, ç”Ÿæˆçš„tokenæ•°)
    """
    # Qwen2.5 çš„èŠå¤©æ¨¡æ¿æ ¼å¼
    formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # é¢„çƒ­
    _ = llm(formatted_prompt, max_tokens=3, echo=False)
    
    # æ­£å¼æ¨ç†å¹¶è®¡æ—¶
    start_time = time.time()
    
    output = llm(
        formatted_prompt,
        max_tokens=max_new_tokens,
        echo=False,
        stop=["<|im_end|>", "<|endoftext|>"]  # åœæ­¢è¯
    )
    
    elapsed = time.time() - start_time
    
    response = output['choices'][0]['text'].strip()
    tokens = output['usage']['completion_tokens']
    
    return response, elapsed, tokens


def print_comparison_result(prompt: str, orig_result: tuple, quant_result: tuple, idx: int):
    """æ‰“å°å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„å¯¹æ¯”ç»“æœ"""
    orig_response, orig_time, orig_tokens = orig_result
    quant_response, quant_time, quant_tokens = quant_result
    
    print(f"\n{'='*80}")
    print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹ {idx}")
    print(f"{'='*80}")
    print(f"\nğŸ”¹ é—®é¢˜: {prompt}")
    
    print(f"\n{'â”€'*80}")
    print(f"ğŸ”µ ã€åŸå§‹æ¨¡å‹ã€‘ Qwen2.5-7B-Instruct (FP32/FP16)")
    print(f"{'â”€'*80}")
    print(f"{orig_response[:300]}..." if len(orig_response) > 300 else orig_response)
    print(f"\n   â±ï¸  è€—æ—¶: {orig_time:.2f}s | Tokens: {orig_tokens} | é€Ÿåº¦: {orig_tokens/orig_time:.1f} tok/s")
    
    print(f"\n{'â”€'*80}")
    print(f"ğŸŸ¢ ã€é‡åŒ–æ¨¡å‹ã€‘ Q4_K_M GGUF (4-bit, llama.cpp + Metal)")
    print(f"{'â”€'*80}")
    print(f"{quant_response[:300]}..." if len(quant_response) > 300 else quant_response)
    print(f"\n   â±ï¸  è€—æ—¶: {quant_time:.2f}s | Tokens: {quant_tokens} | é€Ÿåº¦: {quant_tokens/quant_time:.1f} tok/s")
    
    speedup = orig_time / quant_time if quant_time > 0 else 0
    print(f"\n   ğŸ“Š åŠ é€Ÿæ¯”: {speedup:.2f}x")


def main():
    parser = argparse.ArgumentParser(description="çœŸå®é‡åŒ–å¯¹æ¯”æµ‹è¯• (llama.cpp)")
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="Transformers æ¨¡å‹ ID")
    parser.add_argument('--gguf_path', type=str, 
                        default="models/Qwen2.5-7B-Instruct-Q4_K_M.gguf",
                        help="GGUF æ¨¡å‹è·¯å¾„")
    parser.add_argument('--max_tokens', type=int, default=200,
                        help="æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤ 200ï¼‰")
    
    args = parser.parse_args()
    device = get_device()
    
    print("\n" + "="*80)
    print("ğŸš€ çœŸå®é‡åŒ–å¯¹æ¯”æµ‹è¯•")
    print("   åŸå§‹æ¨¡å‹ (Transformers) vs Q4_K_M é‡åŒ– (llama.cpp + Metal)")
    print("="*80)
    print(f"\nğŸ“ è®¾å¤‡: {device}")
    print(f"ğŸ“¦ åŸå§‹æ¨¡å‹: {args.model_id}")
    print(f"ğŸ“¦ é‡åŒ–æ¨¡å‹: {args.gguf_path}")
    
    # ========== åŠ è½½åŸå§‹æ¨¡å‹ ==========
    print("\n" + "â”€"*80)
    print("â³ æ­£åœ¨åŠ è½½åŸå§‹æ¨¡å‹ (Transformers)...")
    
    if device == "mps":
        original_model = AutoModelForCausalLM.from_pretrained(
            args.model_id, 
            torch_dtype=torch.float32
        )
        original_model = original_model.to("mps")
    else:
        original_model = AutoModelForCausalLM.from_pretrained(
            args.model_id, 
            torch_dtype=torch.float16, 
            device_map="auto"
        )
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
    original_model.eval()
    
    # ä¼°ç®—å†…å­˜
    total_params = sum(p.numel() for p in original_model.parameters())
    orig_memory = total_params * 4 / 1e9 if device == "mps" else total_params * 2 / 1e9
    print(f"âœ… åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆ | å‚æ•°: {total_params/1e9:.2f}B | å†…å­˜: ~{orig_memory:.1f} GB")
    
    # ========== åŠ è½½é‡åŒ–æ¨¡å‹ ==========
    print("\nâ³ æ­£åœ¨åŠ è½½é‡åŒ–æ¨¡å‹ (llama.cpp)...")
    
    try:
        from llama_cpp import Llama
        
        import os
        gguf_path = args.gguf_path
        if not os.path.exists(gguf_path):
            # å°è¯•å…¶ä»–è·¯å¾„
            alt_paths = [
                "models/Qwen2.5-7B-Instruct-Q4_K_M.gguf",
                "./Qwen2.5-7B-Instruct-Q4_K_M.gguf",
                os.path.expanduser("~/.cache/huggingface/hub/models--bartowski--Qwen2.5-7B-Instruct-GGUF/snapshots/*/Qwen2.5-7B-Instruct-Q4_K_M.gguf"),
            ]
            for path in alt_paths:
                import glob
                matches = glob.glob(path)
                if matches:
                    gguf_path = matches[0]
                    break
        
        if not os.path.exists(gguf_path):
            print(f"âŒ GGUF æ¨¡å‹æœªæ‰¾åˆ°: {gguf_path}")
            print("è¯·å…ˆä¸‹è½½æ¨¡å‹:")
            print("huggingface-cli download bartowski/Qwen2.5-7B-Instruct-GGUF Qwen2.5-7B-Instruct-Q4_K_M.gguf --local-dir models")
            return
        
        # åŠ è½½ llama.cpp æ¨¡å‹
        quant_model = Llama(
            model_path=gguf_path,
            n_ctx=4096,        # ä¸Šä¸‹æ–‡é•¿åº¦
            n_gpu_layers=-1,   # ä½¿ç”¨æ‰€æœ‰ GPU å±‚ (Metal)
            n_threads=8,       # CPU çº¿ç¨‹æ•°
            verbose=False
        )
        
        # GGUF æ–‡ä»¶å¤§å°å³å†…å­˜å ç”¨
        quant_memory = os.path.getsize(gguf_path) / 1e9
        print(f"âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ | æ ¼å¼: Q4_K_M | å†…å­˜: ~{quant_memory:.1f} GB")
        print(f"   ğŸ’¾ å†…å­˜èŠ‚çœ: {(1 - quant_memory/orig_memory)*100:.1f}%")
        
    except ImportError:
        print("âŒ llama-cpp-python æœªå®‰è£…")
        print("è¯·è¿è¡Œ: CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python")
        return
    except Exception as e:
        print(f"âŒ åŠ è½½é‡åŒ–æ¨¡å‹å¤±è´¥: {e}")
        return
    
    # ========== æµ‹è¯•ç”¨ä¾‹ ==========
    prompts = [
        "1+1ç­‰äºå¤šå°‘ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿç”¨ä¸€å¥è¯è§£é‡Šã€‚",
        "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚",
        "è¯·ç®€è¦ä»‹ç»å¤ªé˜³ç³»çš„å…«å¤§è¡Œæ˜Ÿã€‚",
        "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿç”¨ç®€å•è¯­è¨€è§£é‡Šã€‚",
    ]
    
    print("\n" + "="*80)
    print("ğŸš€ å¼€å§‹å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    # æ”¶é›†ç»Ÿè®¡æ•°æ®
    total_orig_time = 0
    total_quant_time = 0
    total_orig_tokens = 0
    total_quant_tokens = 0
    
    for idx, prompt in enumerate(prompts, 1):
        # åŸå§‹æ¨¡å‹æ¨ç†
        orig_result = generate_with_transformers(
            original_model, tokenizer, prompt, device, 
            max_new_tokens=args.max_tokens
        )
        
        # é‡åŒ–æ¨¡å‹æ¨ç†
        quant_result = generate_with_llamacpp(
            quant_model, prompt, 
            max_new_tokens=args.max_tokens
        )
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        print_comparison_result(prompt, orig_result, quant_result, idx)
        
        # ç´¯è®¡ç»Ÿè®¡
        total_orig_time += orig_result[1]
        total_quant_time += quant_result[1]
        total_orig_tokens += orig_result[2]
        total_quant_tokens += quant_result[2]
    
    # ========== æ€»ç»“ç»Ÿè®¡ ==========
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    print(f"\nğŸ”µ åŸå§‹æ¨¡å‹ (Transformers, {'FP32' if device == 'mps' else 'FP16'}):")
    print(f"   å†…å­˜å ç”¨: ~{orig_memory:.1f} GB")
    print(f"   æ€»è€—æ—¶: {total_orig_time:.2f}s")
    print(f"   æ€»Tokens: {total_orig_tokens}")
    print(f"   å¹³å‡é€Ÿåº¦: {total_orig_tokens/total_orig_time:.1f} tok/s")
    
    print(f"\nğŸŸ¢ é‡åŒ–æ¨¡å‹ (llama.cpp, Q4_K_M):")
    print(f"   å†…å­˜å ç”¨: ~{quant_memory:.1f} GB")
    print(f"   æ€»è€—æ—¶: {total_quant_time:.2f}s")
    print(f"   æ€»Tokens: {total_quant_tokens}")
    print(f"   å¹³å‡é€Ÿåº¦: {total_quant_tokens/total_quant_time:.1f} tok/s")
    
    avg_speedup = total_orig_time / total_quant_time if total_quant_time > 0 else 0
    memory_saving = (1 - quant_memory/orig_memory) * 100
    
    print(f"\n{'â”€'*80}")
    print(f"ğŸ“ˆ å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
    print(f"ğŸ’¾ å†…å­˜èŠ‚çœ: {memory_saving:.1f}%")
    print(f"{'â”€'*80}")
    
    if avg_speedup > 1:
        print(f"\nâœ… é‡åŒ–æ¨¡å‹æ¯”åŸå§‹æ¨¡å‹å¿« {avg_speedup:.1f} å€!")
    else:
        print(f"\nâš ï¸  é‡åŒ–æ¨¡å‹è¾ƒæ…¢ï¼Œå¯èƒ½æ˜¯å› ä¸º Metal ä¼˜åŒ–é—®é¢˜")
    
    print("\n" + "="*80)
    print("âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()

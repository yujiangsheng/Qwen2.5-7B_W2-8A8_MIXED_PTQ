"""
ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•è„šæœ¬
==================

åŠŸèƒ½è¯´æ˜ï¼š
---------
å¯¹æ¯”ä¸‰ç§æ¨¡å‹çš„æ¨ç†æ€§èƒ½å’Œè¾“å‡ºè´¨é‡ï¼š
    1. åŸå§‹æ¨¡å‹ (FP32/FP16) - ä½¿ç”¨ Transformers åº“
    2. Q4_K_M ç»Ÿä¸€é‡åŒ– (4-bit) - ä½¿ç”¨ llama.cpp (GGUFæ ¼å¼)
    3. æ··åˆç²¾åº¦é‡åŒ– (W2/W4/W8) - ä½¿ç”¨ llama.cpp (è‡ªå®šä¹‰GGUF)

æµ‹è¯•æŒ‡æ ‡ï¼š
---------
    - æ¨ç†é€Ÿåº¦ (tokens/second)
    - ç”Ÿæˆè´¨é‡ (è¾“å‡ºæ–‡æœ¬å¯¹æ¯”)
    - æ¨¡å‹å¤§å° (GB)
    - å†…å­˜å ç”¨

ä½¿ç”¨æ–¹æ³•ï¼š
---------
    # åŸºç¡€ç”¨æ³•ï¼ˆè·³è¿‡åŸå§‹æ¨¡å‹ä»¥èŠ‚çœå†…å­˜ï¼‰
    >>> python compare_three_models.py --skip_original
    
    # å®Œæ•´å¯¹æ¯”ï¼ˆéœ€è¦è¶³å¤Ÿå†…å­˜ï¼‰
    >>> python compare_three_models.py --max_tokens 200
    
    # è‡ªå®šä¹‰ç”Ÿæˆé•¿åº¦
    >>> python compare_three_models.py --max_tokens 300 --skip_original

ä½œè€…ï¼šJiangsheng Yu
"""

import torch
import time
import argparse
import os
from transformers import AutoModelForCausalLM, AutoTokenizer

# ============================================================================
# ä¾èµ–æ£€æŸ¥
# ============================================================================
# llama-cpp-python æ˜¯ç”¨äºåŠ è½½å’Œè¿è¡Œ GGUF æ ¼å¼æ¨¡å‹çš„åº“
# å®ƒæä¾›äº†é«˜æ•ˆçš„ CPU/GPU æ¨ç†èƒ½åŠ›
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    print("âš ï¸  llama-cpp-python æœªå®‰è£…ï¼Œå°†è·³è¿‡ GGUF æ¨¡å‹æµ‹è¯•")
    print("   å®‰è£…å‘½ä»¤:")
    print("   - macOS (Metal): CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python")
    print("   - Linux (CUDA):  CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python")


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def get_device() -> str:
    """
    è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¡ç®—è®¾å¤‡
    
    æ£€æµ‹é¡ºåºï¼š
        1. CUDA (NVIDIA GPU)
        2. MPS (Apple Silicon)
        3. CPU (é€šç”¨åå¤‡)
    
    è¿”å›:
        str: è®¾å¤‡åç§° ('cuda', 'mps', æˆ– 'cpu')
    """
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def generate_with_transformers(model, tokenizer, prompt: str, device: str,
                                max_new_tokens: int = 200) -> tuple:
    """
    ä½¿ç”¨ HuggingFace Transformers åº“ç”Ÿæˆå›å¤
    
    å‚æ•°:
        model: å·²åŠ è½½çš„ Transformers æ¨¡å‹
        tokenizer: å¯¹åº”çš„åˆ†è¯å™¨
        prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºæ–‡æœ¬
        device: è®¡ç®—è®¾å¤‡ ('cuda', 'mps', 'cpu')
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°é‡ï¼ˆé»˜è®¤200ï¼‰
    
    è¿”å›:
        tuple: (ç”Ÿæˆçš„æ–‡æœ¬, è€—æ—¶ç§’æ•°, ç”Ÿæˆçš„tokenæ•°)
    
    æ³¨æ„:
        - ä¼šå…ˆè¿›è¡Œä¸€æ¬¡é¢„çƒ­æ¨ç†ä»¥è·å¾—æ›´å‡†ç¡®çš„æ—¶é—´æµ‹é‡
        - ä½¿ç”¨ greedy decoding (do_sample=False) ç¡®ä¿ç»“æœå¯å¤ç°
    """
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(text, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # é¢„çƒ­
    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=3, pad_token_id=tokenizer.eos_token_id)
    
    # æ­£å¼æ¨ç†
    start_time = time.time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    elapsed = time.time() - start_time
    
    new_tokens = outputs.shape[1] - inputs['input_ids'].shape[1]
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:],
        skip_special_tokens=True
    )
    
    return response, elapsed, new_tokens


def generate_with_llama_cpp(model: "Llama", prompt: str,
                             max_tokens: int = 200) -> tuple:
    """
    ä½¿ç”¨ llama.cpp ç”Ÿæˆå›å¤
    
    å‚æ•°:
        model: å·²åŠ è½½çš„ Llama æ¨¡å‹å®ä¾‹
        prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºæ–‡æœ¬
        max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°é‡ï¼ˆé»˜è®¤200ï¼‰
    
    è¿”å›:
        tuple: (ç”Ÿæˆçš„æ–‡æœ¬, è€—æ—¶ç§’æ•°, ç”Ÿæˆçš„tokenæ•°)
    
    ç‰¹ç‚¹:
        - llama.cpp ä½¿ç”¨ä¼˜åŒ–çš„ C++ åç«¯ï¼Œæ¨ç†é€Ÿåº¦å¿«
        - æ”¯æŒ Metal (macOS) å’Œ CUDA (Linux/Windows) åŠ é€Ÿ
        - ä½¿ç”¨ temperature=0.0 ç¡®ä¿è¾“å‡ºç¡®å®šæ€§
    """
    # é¢„çƒ­ï¼šé¦–æ¬¡æ¨ç†å¯èƒ½è¾ƒæ…¢ï¼Œä¸è®¡å…¥æµ‹é‡
    _ = model.create_chat_completion(
        messages=[{"role": "user", "content": "hi"}],
        max_tokens=3
    )
    
    # æ­£å¼æ¨ç†
    start_time = time.time()
    response = model.create_chat_completion(
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=0.0,
    )
    elapsed = time.time() - start_time
    
    content = response['choices'][0]['message']['content']
    tokens = response['usage']['completion_tokens']
    
    return content, elapsed, tokens


def print_result(name: str, response: str, elapsed: float, tokens: int, 
                 color_code: str = ""):
    """
    æ ¼å¼åŒ–æ‰“å°å•ä¸ªæ¨¡å‹çš„æ¨ç†ç»“æœ
    
    å‚æ•°:
        name: æ¨¡å‹åç§°
        response: ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        elapsed: æ¨ç†è€—æ—¶ï¼ˆç§’ï¼‰
        tokens: ç”Ÿæˆçš„ token æ•°é‡
        color_code: å¯é€‰çš„é¢œè‰²/å›¾æ ‡å‰ç¼€
    """
    print(f"\n{'â”€'*80}")
    print(f"{color_code}ã€{name}ã€‘")
    print(f"{'â”€'*80}")
    # æ˜¾ç¤ºæ›´å¤šæ–‡æœ¬å†…å®¹ï¼ˆæœ€å¤š600å­—ç¬¦ï¼‰ï¼Œä¾¿äºå¯¹æ¯”è¾“å‡ºè´¨é‡
    print(f"{response[:600]}{'...' if len(response) > 600 else ''}")
    print(f"\n   â±ï¸  è€—æ—¶: {elapsed:.2f}s | Tokens: {tokens} | é€Ÿåº¦: {tokens/elapsed:.1f} tok/s")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•
    
    æµ‹è¯•æµç¨‹ï¼š
        1. åŠ è½½æ‰€æœ‰å¯ç”¨æ¨¡å‹
        2. å¯¹æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹æ‰§è¡Œæ¨ç†
        3. è®°å½•å¹¶å¯¹æ¯”æ€§èƒ½æŒ‡æ ‡
        4. è¾“å‡ºæ€»ç»“æŠ¥å‘Š
    """
    # ---- å‘½ä»¤è¡Œå‚æ•°è§£æ ----
    parser = argparse.ArgumentParser(
        description="ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•ï¼šåŸå§‹æ¨¡å‹ vs Q4_K_M vs æ··åˆç²¾åº¦",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python compare_three_models.py --skip_original --max_tokens 200
  python compare_three_models.py --max_tokens 300
        """
    )
    parser.add_argument('--model_id', type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="HuggingFace æ¨¡å‹ IDï¼ˆåŸå§‹æ¨¡å‹ï¼‰")
    parser.add_argument('--q4km_path', type=str, 
                        default="models/Qwen2.5-7B-Instruct-Q4_K_M.gguf",
                        help="Q4_K_M é‡åŒ–æ¨¡å‹è·¯å¾„ (GGUFæ ¼å¼)")
    parser.add_argument('--mixed_path', type=str,
                        default="models/qwen2.5-7b-mixed.gguf",
                        help="æ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹è·¯å¾„ (GGUFæ ¼å¼)")
    parser.add_argument('--max_tokens', type=int, default=200,
                        help="æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤200ï¼Œå»ºè®®ä¸å°‘äº100ï¼‰")
    parser.add_argument('--skip_original', action='store_true',
                        help="è·³è¿‡åŸå§‹æ¨¡å‹æµ‹è¯•ï¼ˆèŠ‚çœå†…å­˜å’Œæ—¶é—´ï¼‰")
    
    args = parser.parse_args()
    device = get_device()
    
    print("\n" + "="*80)
    print("ğŸš€ ä¸‰æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("   åŸå§‹æ¨¡å‹ vs Q4_K_M vs æ··åˆç²¾åº¦")
    print("="*80)
    print(f"\nğŸ“ è®¾å¤‡: {device}")
    print(f"ğŸ“¦ åŸå§‹æ¨¡å‹: {args.model_id}")
    print(f"ğŸ“¦ Q4_K_M: {args.q4km_path}")
    print(f"ğŸ“¦ æ··åˆç²¾åº¦: {args.mixed_path}")
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(args.q4km_path):
        print(f"\nâŒ æ‰¾ä¸åˆ° Q4_K_M æ¨¡å‹: {args.q4km_path}")
        return
    if not os.path.exists(args.mixed_path):
        print(f"\nâŒ æ‰¾ä¸åˆ°æ··åˆç²¾åº¦æ¨¡å‹: {args.mixed_path}")
        return
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
    q4km_size = os.path.getsize(args.q4km_path) / (1024**3)
    mixed_size = os.path.getsize(args.mixed_path) / (1024**3)
    print(f"\nğŸ“Š æ¨¡å‹å¤§å°:")
    print(f"   Q4_K_M: {q4km_size:.2f} GB")
    print(f"   æ··åˆç²¾åº¦: {mixed_size:.2f} GB")
    
    # ========== åŠ è½½æ¨¡å‹ ==========
    print("\n" + "â”€"*80)
    print("â³ åŠ è½½æ¨¡å‹...")
    
    models = {}
    
    # 1. åŸå§‹æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    if not args.skip_original:
        print("\nğŸ“¥ åŠ è½½åŸå§‹æ¨¡å‹ (Transformers)...")
        tokenizer = AutoTokenizer.from_pretrained(args.model_id)
        if device == "mps":
            original_model = AutoModelForCausalLM.from_pretrained(
                args.model_id,
                torch_dtype=torch.float32
            ).to(device)
        else:
            original_model = AutoModelForCausalLM.from_pretrained(
                args.model_id,
                torch_dtype=torch.float16
            ).to(device)
        original_model.eval()
        models['original'] = (original_model, tokenizer)
        print("âœ… åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 2. Q4_K_M æ¨¡å‹
    if LLAMA_CPP_AVAILABLE:
        print("\nğŸ“¥ åŠ è½½ Q4_K_M æ¨¡å‹ (llama.cpp)...")
        q4km_model = Llama(
            model_path=args.q4km_path,
            n_ctx=4096,
            n_gpu_layers=-1,
            verbose=False
        )
        models['q4km'] = q4km_model
        print("âœ… Q4_K_M æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 3. æ··åˆç²¾åº¦æ¨¡å‹
        print("\nğŸ“¥ åŠ è½½æ··åˆç²¾åº¦æ¨¡å‹ (llama.cpp)...")
        try:
            mixed_model = Llama(
                model_path=args.mixed_path,
                n_ctx=4096,
                n_gpu_layers=-1,
                verbose=False
            )
            models['mixed'] = mixed_model
            print("âœ… æ··åˆç²¾åº¦æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  æ··åˆç²¾åº¦æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   è¿™å¯èƒ½æ˜¯å› ä¸ºç®€åŒ–çš„ GGUF æ ¼å¼ä¸ llama.cpp ä¸å®Œå…¨å…¼å®¹")
            models['mixed'] = None
    
    # ========== æµ‹è¯•ç”¨ä¾‹ ==========
    test_prompts = [
        "1+1ç­‰äºå¤šå°‘ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯Transformeræ¶æ„ï¼Ÿç”¨ä¸€å¥è¯è§£é‡Šã€‚",
        "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ã€‚",
        "è¯·ç®€è¦ä»‹ç»å¤ªé˜³ç³»çš„å…«å¤§è¡Œæ˜Ÿã€‚",
        "ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿç”¨ç®€å•è¯­è¨€è§£é‡Šã€‚"
    ]
    
    print("\n" + "="*80)
    print("ğŸš€ å¼€å§‹å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    # ç»Ÿè®¡æ•°æ®
    stats = {
        'original': {'time': 0, 'tokens': 0},
        'q4km': {'time': 0, 'tokens': 0},
        'mixed': {'time': 0, 'tokens': 0}
    }
    
    for idx, prompt in enumerate(test_prompts, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹ {idx}")
        print(f"{'='*80}")
        print(f"\nğŸ”¹ é—®é¢˜: {prompt}")
        
        results = {}
        
        # åŸå§‹æ¨¡å‹
        if 'original' in models:
            model, tokenizer = models['original']
            resp, elapsed, tokens = generate_with_transformers(
                model, tokenizer, prompt, device, args.max_tokens
            )
            results['original'] = (resp, elapsed, tokens)
            stats['original']['time'] += elapsed
            stats['original']['tokens'] += tokens
            print_result("åŸå§‹æ¨¡å‹ (FP32/FP16)", resp, elapsed, tokens, "ğŸ”µ ")
        
        # Q4_K_M
        if 'q4km' in models and LLAMA_CPP_AVAILABLE:
            resp, elapsed, tokens = generate_with_llama_cpp(
                models['q4km'], prompt, args.max_tokens
            )
            results['q4km'] = (resp, elapsed, tokens)
            stats['q4km']['time'] += elapsed
            stats['q4km']['tokens'] += tokens
            print_result("Q4_K_M (4-bit ç»Ÿä¸€é‡åŒ–)", resp, elapsed, tokens, "ğŸŸ¢ ")
        
        # æ··åˆç²¾åº¦
        if models.get('mixed') is not None:
            try:
                resp, elapsed, tokens = generate_with_llama_cpp(
                    models['mixed'], prompt, args.max_tokens
                )
                results['mixed'] = (resp, elapsed, tokens)
                stats['mixed']['time'] += elapsed
                stats['mixed']['tokens'] += tokens
                print_result("æ··åˆç²¾åº¦ (W2/W4/W8)", resp, elapsed, tokens, "ğŸŸ¡ ")
            except Exception as e:
                print(f"\nâš ï¸  æ··åˆç²¾åº¦æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        # é€Ÿåº¦å¯¹æ¯”
        if len(results) >= 2:
            print(f"\n{'â”€'*80}")
            print("ğŸ“Š é€Ÿåº¦å¯¹æ¯”:")
            
            if 'original' in results and 'q4km' in results:
                speedup = results['original'][1] / results['q4km'][1]
                print(f"   Q4_K_M vs åŸå§‹: {speedup:.2f}x åŠ é€Ÿ")
            
            if 'original' in results and 'mixed' in results:
                speedup = results['original'][1] / results['mixed'][1]
                print(f"   æ··åˆç²¾åº¦ vs åŸå§‹: {speedup:.2f}x åŠ é€Ÿ")
            
            if 'q4km' in results and 'mixed' in results:
                ratio = results['q4km'][1] / results['mixed'][1]
                print(f"   æ··åˆç²¾åº¦ vs Q4_K_M: {ratio:.2f}x")
    
    # ========== æ€»ç»“ ==========
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ æ¨¡å‹                 â”‚ å¤§å°         â”‚ æ€»è€—æ—¶   â”‚ å¹³å‡é€Ÿåº¦     â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    if 'original' in models:
        orig_speed = stats['original']['tokens'] / stats['original']['time'] if stats['original']['time'] > 0 else 0
        print(f"â”‚ åŸå§‹ (FP32/FP16)    â”‚ ~14.2 GB     â”‚ {stats['original']['time']:6.2f}s  â”‚ {orig_speed:6.1f} tok/s  â”‚")
    
    if 'q4km' in models:
        q4km_speed = stats['q4km']['tokens'] / stats['q4km']['time'] if stats['q4km']['time'] > 0 else 0
        print(f"â”‚ Q4_K_M (4-bit)      â”‚ {q4km_size:5.2f} GB     â”‚ {stats['q4km']['time']:6.2f}s  â”‚ {q4km_speed:6.1f} tok/s  â”‚")
    
    if models.get('mixed') is not None and stats['mixed']['time'] > 0:
        mixed_speed = stats['mixed']['tokens'] / stats['mixed']['time']
        print(f"â”‚ æ··åˆç²¾åº¦ (W2/W4/W8) â”‚ {mixed_size:5.2f} GB     â”‚ {stats['mixed']['time']:6.2f}s  â”‚ {mixed_speed:6.1f} tok/s  â”‚")
    
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # å¯¹æ¯”åˆ†æ
    print("\nğŸ“ˆ å¯¹æ¯”åˆ†æ:")
    
    if 'original' in models and stats['q4km']['time'] > 0:
        speedup = stats['original']['time'] / stats['q4km']['time']
        print(f"   â€¢ Q4_K_M æ¯”åŸå§‹æ¨¡å‹å¿« {speedup:.1f}xï¼Œå¤§å°å‡å°‘ {(1-q4km_size/14.2)*100:.0f}%")
    
    if models.get('mixed') is not None and stats['mixed']['time'] > 0:
        if 'original' in models:
            speedup = stats['original']['time'] / stats['mixed']['time']
            print(f"   â€¢ æ··åˆç²¾åº¦æ¯”åŸå§‹æ¨¡å‹å¿« {speedup:.1f}xï¼Œå¤§å°å‡å°‘ {(1-mixed_size/14.2)*100:.0f}%")
        
        if stats['q4km']['time'] > 0:
            ratio = stats['q4km']['time'] / stats['mixed']['time']
            size_diff = mixed_size - q4km_size
            if ratio > 1:
                print(f"   â€¢ æ··åˆç²¾åº¦æ¯” Q4_K_M å¿« {ratio:.2f}xï¼Œä½†å¤§å°å¢åŠ  {size_diff:.2f} GB")
            else:
                print(f"   â€¢ æ··åˆç²¾åº¦æ¯” Q4_K_M æ…¢ {1/ratio:.2f}xï¼Œå¤§å°å¢åŠ  {size_diff:.2f} GB")
    
    print("\n" + "="*80)
    print("âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    print("="*80)


if __name__ == "__main__":
    main()
